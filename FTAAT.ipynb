{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import random\n",
    "import string\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scripts.fact_gen import generate_facts_k_tokens\n",
    "from scripts.build_prompt import build_prompt_for_all_keys\n",
    "\n",
    "\n",
    "load_dotenv() # Get dat api key\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Log Directory\n",
    "LOG_DIR = Path(\"experiment_logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "ENCODING_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "# For token counting\n",
    "encoding = tiktoken.encoding_for_model(ENCODING_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gpt_tokens(text: str) -> int:\n",
    "    \"\"\"Return how many tokens `text` uses under the GPT-3.5 (or GPT-4) tokenizer.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_gpt(prompt: str, model: str = MODEL_NAME, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with the given prompt,\n",
    "    returns the assistant's message content.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        instructions=\"You are a helpful assistant. Respond with only the correct answer on each line.\",\n",
    "        input=prompt,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Token Set\n",
    "filename = \"data/tokens/alpha_tokens.json\"\n",
    "with open(filename, 'r') as f:\n",
    "        token_set = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(facts_list_sizes=[5, 10], token_sizes=[2, 3], trials=3):\n",
    "    \"\"\"Enhanced version with comprehensive logging\"\"\"\n",
    "    single_token_vocab = list(token_set)\n",
    "    \n",
    "    records = []\n",
    "    for num_facts in facts_list_sizes:\n",
    "        for k in token_sizes:\n",
    "            for trial_idx in range(trials):\n",
    "                # Generate facts and prompt\n",
    "                facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "                prompt_text, question_keys = build_prompt_for_all_keys(facts_list)\n",
    "                prompt_len = count_gpt_tokens(prompt_text)\n",
    "                \n",
    "                # Query GPT\n",
    "                try:\n",
    "                    response = query_gpt(prompt_text, model=MODEL_NAME, temperature=0.0)\n",
    "                except Exception as e:\n",
    "                    response = f\"ERROR: {str(e)}\"\n",
    "                \n",
    "                # Grade response\n",
    "                accuracy, num_matches, total_questions, mismatches = grade_response(\n",
    "                    response, question_keys, key_value_dict\n",
    "                )\n",
    "                \n",
    "                # Build complete record\n",
    "                record = {\n",
    "                    \"num_facts\": num_facts,\n",
    "                    \"k\": k,\n",
    "                    \"trial\": trial_idx,\n",
    "                    \"prompt_text\": prompt_text,  # Store actual prompt\n",
    "                    \"question_keys\": question_keys,  # Store expected questions\n",
    "                    \"key_value_dict\": key_value_dict,  # Store correct answers\n",
    "                    \"prompt_len\": prompt_len,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"num_correct\": num_matches,\n",
    "                    \"num_questions\": total_questions,\n",
    "                    \"mismatches\": mismatches,\n",
    "                    \"response\": response,\n",
    "                }\n",
    "                \n",
    "                # Log everything\n",
    "                #log_experiment(record)\n",
    "                # if accuracy < 1:\n",
    "                #     log_mismatch_details(record)\n",
    "                \n",
    "                records.append(record)\n",
    "                print(f\"Completed: facts={num_facts}, k={k}, trial={trial_idx}, accuracy={accuracy:.2f}\")\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage (Uncomment once you have your API key set):\n",
    "#results_df = run_experiments(facts_list_sizes=[50, 100, 150 ,200, 250,300,350,400,450], token_sizes=[1,2,3,4], trials=4)\n",
    "#results_df = run_experiments(facts_list_sizes=[2,5,10,25,50,100], token_sizes=[1,2,5,10,25], trials=10)\n",
    "#results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-token vocab size: 10000\n",
      "=== Prompt (Debug) ===\n",
      "\n",
      "Below are some arbitrary assignments of Key => Value:\n",
      "\n",
      "ligne|kill|untu => play|jobs|ieme\n",
      "def|reich|blk => ampus|ale|aval\n",
      "plans|pg|forc => rical|reeze|kre\n",
      "parm|oure|oglob => ison|shops|ammad\n",
      "ene|aper|ive => inned|orra|ackle\n",
      "\n",
      "Your task is to provide the correct Values for each of the following Keys, in the exact same order they are listed:\n",
      "\n",
      "1) plans|pg|forc\n",
      "2) def|reich|blk\n",
      "3) ene|aper|ive\n",
      "4) parm|oure|oglob\n",
      "5) ligne|kill|untu\n",
      "\n",
      "Answer with one value per line, matching the order of the keys above.\n",
      "\n",
      "Prompt token length: 154\n",
      "\n",
      "=== The correct ordering of values (reference) ===\n",
      "rical|reeze|kre\n",
      "ampus|ale|aval\n",
      "inned|orra|ackle\n",
      "ison|shops|ammad\n",
      "play|jobs|ieme\n",
      "\n",
      "=== GPT RESPONSE ===\n",
      "1) rical|reeze|kre\n",
      "2) ampus|ale|aval\n",
      "3) inned|orra|ackle\n",
      "4) ison|shops|ammad\n",
      "5) play|jobs|ieme\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE PROMPT AND RESPONSE\n",
    "single_token_vocab = list(token_set)\n",
    "print(\"Single-token vocab size:\", len(single_token_vocab))\n",
    "\n",
    "# Generate some facts\n",
    "num_facts = 5\n",
    "k = 3  # each key = 3 tokens, each value = 3 tokens\n",
    "facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "\n",
    "# Build the big prompt that queries all keys in random order\n",
    "prompt, question_keys_in_order = build_prompt_for_all_keys(facts_list)\n",
    "\n",
    "print(\"=== Prompt (Debug) ===\\n\")\n",
    "print(prompt)\n",
    "prompt_len = count_gpt_tokens(prompt)\n",
    "print(f\"\\nPrompt token length: {prompt_len}\")\n",
    "\n",
    "print(\"\\n=== The correct ordering of values (reference) ===\")\n",
    "for key in question_keys_in_order:\n",
    "    print(f\"{key_value_dict[key]}\")\n",
    "\n",
    "# Query GPT (uncomment if you have your key set up)\n",
    "answer = query_gpt(prompt, model=MODEL_NAME, temperature=0.0)\n",
    "print(\"\\n=== GPT RESPONSE ===\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
