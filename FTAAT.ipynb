{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V0 First Implementation Notebook\n",
    "##### Supports only openAI API: gpt-3.5-turbo\n",
    "##### Includes test functions for debugging\n",
    "##### After finishing this chatgpt helpd convert it to a script and add the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To manually run newer (run_experiments.py) version please refer to the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total files deleted: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "FOLDER_PATH = \"results/prompt_default_prompt/openai\"  # <-- Change this to your actual folder\n",
    "ERROR_MESSAGE = (\n",
    "    \"ERROR: 'OpenAIProvider' object has no attribute 'query'\"\n",
    ")\n",
    "\n",
    "def should_delete(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for trial in data.get(\"trials\", []):\n",
    "                if trial.get(\"response_text\") == ERROR_MESSAGE:\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error reading {file_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "def delete_matching_files(folder):\n",
    "    deleted_count = 0\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for name in files:\n",
    "            if name.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, name)\n",
    "                if should_delete(file_path):\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"[✓] Deleted: {file_path}\")\n",
    "                        deleted_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Failed to delete {file_path}: {e}\")\n",
    "    print(f\"\\nTotal files deleted: {deleted_count}\")\n",
    "\n",
    "# Run it\n",
    "delete_matching_files(FOLDER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Batch ID batch_681d2c123578819084e793eaa3160285  (1.0 MB) submitted\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m80\u001b[39m):\n\u001b[32m     13\u001b[39m     ott.append(i)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm_providers.openai_llm.OpenAIProvider\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfacts_list_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43motf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mott\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_root\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults/prompt_default_prompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# run_experiments(\"llm_providers.openai_llm.OpenAIProvider\",\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#                 facts_list_sizes=[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30], token_sizes=[10,15,20,25,30,35,40,45,50],trials=3,output_root=\"results/prompt_default_prompt\",batch_size = 500,timeout_sec = 10000)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# run_experiments(\"llm_providers.openai_llm.OpenAIProvider\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# \"llm_providers.deepseek_llm.DeepSeekProvider\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# \"llm_providers.ollama_llm.OllamaProvider\" In progress\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AGI2/FTAAT/scripts/run_experiments.py:124\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m(provider_module, facts_list_sizes, token_sizes, trials, output_root, prompt_id, verbose, adaptive, early_abort, timeout_sec, max_tok_mult, batch_size)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# Flush if batch limit reached\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pending_batch) >= batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[43mflush_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         pending_batch.clear()\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AGI2/FTAAT/scripts/run_experiments.py:265\u001b[39m, in \u001b[36mflush_batch\u001b[39m\u001b[34m(llm, batch_items, base_dir, prompt_id)\u001b[39m\n\u001b[32m    263\u001b[39m result_path = os.path.join(base_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch_output_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(result_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     f_out.write(\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_id\u001b[49m\u001b[43m)\u001b[49m.read())\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# parse & record (same logic as before) ---------------------------\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(result_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_in:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AGI2/FTAAT/venv/lib/python3.12/site-packages/openai/resources/files.py:279\u001b[39m, in \u001b[36mFiles.content\u001b[39m\u001b[34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03mReturns the contents of the specified file.\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_id:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/binary\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get(\n\u001b[32m    282\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/content\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    283\u001b[39m     options=make_request_options(\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     cast_to=_legacy_response.HttpxBinaryResponseContent,\n\u001b[32m    287\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "from scripts.run_experiments import run_experiments\n",
    "# run_experiments(\"llm_providers.deepseek_llm.DeepSeekProvider\",\n",
    "#                 facts_list_sizes=[10,20,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180], token_sizes=[10,15,20,25,30,35,40,45,50,55,60,65,70],trials=3,output_root=\"results/dpprompt_default_prompt\")\n",
    "# run_experiments(\"llm_providers.deepseek_llm.DeepSeekProvider\",\n",
    "#                 facts_list_sizes=[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30], token_sizes=[10,15,20,25,30,35,40,45,50],trials=3,output_root=\"results/dpprompt_default_prompt\")\n",
    "# run_experiments(\"llm_providers.deepseek_llm.DeepSeekProvider\",\n",
    "#                 facts_list_sizes=[10,20,40,50,60,70,80,90,100,110,120,130,140,150], token_sizes=[1,2,3,4,5,6,7,8,9,10],trials=3,output_root=\"results/dpprompt_default_prompt\")\n",
    "otf = []\n",
    "ott = []\n",
    "for i in range(1, 130):\n",
    "    otf.append(i)\n",
    "for i in range(1, 80):\n",
    "    ott.append(i)\n",
    "run_experiments(\"llm_providers.openai_llm.OpenAIProvider\",\n",
    "                facts_list_sizes=otf, token_sizes=ott,trials=3,output_root=\"results/prompt_default_prompt\",batch_size = 100,timeout_sec = 10000)\n",
    "# run_experiments(\"llm_providers.openai_llm.OpenAIProvider\",\n",
    "#                 facts_list_sizes=[2,4,6,8,10,12,14,16,18,20,22,24,26,28,30], token_sizes=[10,15,20,25,30,35,40,45,50],trials=3,output_root=\"results/prompt_default_prompt\",batch_size = 500,timeout_sec = 10000)\n",
    "# run_experiments(\"llm_providers.openai_llm.OpenAIProvider\",\n",
    "#                 facts_list_sizes=[10,20,40,50,60,70,80,90,100,110,120,130,140,150], token_sizes=[1,2,3,4,5,6,7,8,9,10],trials=3,output_root=\"results/prompt_default_prompt\",batch_size = 500,timeout_sec = 10000)\n",
    "                \n",
    "# Supported Providers\n",
    "# \"llm_providers.openai_llm.OpenAIProvider\"\n",
    "# \"llm_providers.deepseek_llm.DeepSeekProvider\"\n",
    "# \"llm_providers.ollama_llm.OllamaProvider\" In progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime # Results & Storage\n",
    "import json # \n",
    "from pathlib import Path # \n",
    "\n",
    "from scripts.fact_gen import generate_facts_k_tokens # Helper Functions\n",
    "from scripts.build_prompt import build_prompt_for_all_keys #\n",
    "from scripts.eval import evaluate_token_sequences # \n",
    "\n",
    "from dotenv import load_dotenv # APIs\n",
    "from openai import OpenAI\n",
    "import tiktoken # Tokenizer\n",
    "\n",
    "load_dotenv() # Get dem api key\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "ENCODING_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "# For token counting\n",
    "encoding = tiktoken.encoding_for_model(ENCODING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gpt_tokens(text: str) -> int:\n",
    "    \"\"\"Return how many tokens `text` uses under the GPT-3.5 (or GPT-4) tokenizer.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_gpt(prompt: str, model: str = MODEL_NAME, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with the given prompt,\n",
    "    returns the assistant's message content.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        instructions=\"You are a helpful assistant. Respond with only the correct answer on each line.\",\n",
    "        input=prompt,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/tokens/alpha_tokens.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Token Set\u001b[39;00m\n\u001b[32m      2\u001b[39m filename = \u001b[33m\"\u001b[39m\u001b[33mdata/tokens/alpha_tokens.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m         token_set = \u001b[38;5;28mset\u001b[39m(json.load(f))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Convert to list and count list\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Marco\\Desktop\\FTAT\\FTAT\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/tokens/alpha_tokens.json'"
     ]
    }
   ],
   "source": [
    "# Load Token Set\n",
    "filename = \"data/tokens/gpt35_tokens_clean.json\"\n",
    "with open(filename, 'r') as f:\n",
    "        token_set = set(json.load(f))\n",
    "\n",
    "# Convert to list and count list\n",
    "single_token_vocab = list(token_set)\n",
    "print(\"Single-token vocab size:\", len(single_token_vocab))\n",
    "\n",
    "# Ensure unique tokens by count_gpt_tokens\n",
    "check_good = True\n",
    "for token in single_token_vocab:\n",
    "    if count_gpt_tokens(token) != 1:\n",
    "        print(f\"Token '{token}' has {count_gpt_tokens(token)} tokens, not 1.\")\n",
    "        check_good = False\n",
    "if check_good:\n",
    "    print(\"All tokens are single tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_prompt_and_response(single_token_vocab,num_facts=5, k=3): # For Debugging/testing\n",
    "    \"\"\"\n",
    "    Example prompt and response using the given single-token vocabulary.\n",
    "    Prints results to the console.\n",
    "    Args:\n",
    "        single_token_vocab (list): List of single-token vocabulary.\n",
    "        num_facts (int): Number of facts to generate.\n",
    "        k (int): Number of tokens per key-value pair.\n",
    "    \"\"\"\n",
    "    facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab) # Generate facts\n",
    "\n",
    "    prompt, question_keys_in_order = build_prompt_for_all_keys(facts_list) # Build prompt  \n",
    "\n",
    "    print(\"=== Prompt===\\n\")\n",
    "    print(prompt)\n",
    "\n",
    "    print(\"\\n=== correct ===\")\n",
    "    for key in question_keys_in_order: # Print the correct answers in order\n",
    "        print(f\"{key_value_dict[key]}\")\n",
    "\n",
    "    answer = query_gpt(prompt, model=MODEL_NAME, temperature=0.0) # Query GPT\n",
    "    print(\"\\n=== GPT RESPONSE ===\")\n",
    "    print(answer)\n",
    "\n",
    "### UNCOMMENT TO RUN EXAMPLE PROMPT AND RESPONSE\n",
    "#example_prompt_and_response(single_token_vocab=single_token_vocab, num_facts=5, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_response(response_text: str, question_keys_in_order: list, key_value_dict: dict, num_facts: int, k: int):\n",
    "    \"\"\"\n",
    "    Grades GPT response:\n",
    "    -- Checks if response starts with a non-supported character\n",
    "    -- Converts response to a list of strings\n",
    "    -- Compares token count with expected token count\n",
    "    -- Compares response with expected response\n",
    "    Returns a tuple of (sequence_accuracy, token_accuracy, major_format_flaw)\n",
    "    \"\"\"\n",
    "    # Check if response starts or ends with a non supported character (a-z lowercase is only supported)\n",
    "    major_format_flaw = False\n",
    "    if not response_text[0].isalpha() or response_text[-1].isalpha():\n",
    "        major_format_flaw = True\n",
    "\n",
    "    # expected response\n",
    "    correct_response_seqs = [key_value_dict[key] for key in question_keys_in_order]\n",
    "\n",
    "    # Parse model response\n",
    "    response_seqs = [line.strip() for line in response_text.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # Token Count (Using Tokenizer)\n",
    "    expected_token_count = sum(count_gpt_tokens(seq) for seq in correct_response_seqs)\n",
    "    response_token_count = sum(count_gpt_tokens(seq) for seq in response_seqs)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = evaluate_token_sequences(response_seqs, correct_response_seqs)\n",
    "    return scores, major_format_flaw, expected_token_count, response_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEBUGGING\n",
    "def grade_test(single_token_vocab, num_facts=5, k=3): \n",
    "    \"\"\"\n",
    "    Generates a single test case,\n",
    "    queries GPT for a response, and grades the response.\n",
    "    Prints the results to the console.\n",
    "    Args:\n",
    "        single_token_vocab (list): List of single-token vocabulary.\n",
    "        num_facts (int): Number of facts to generate.\n",
    "        k (int): Number of tokens per key-value pair.\n",
    "    \"\"\"\n",
    "    facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "    prompt, question_keys_in_order = build_prompt_for_all_keys(facts_list)\n",
    "    answer = query_gpt(prompt, model=MODEL_NAME, temperature=0.0)\n",
    "    (response_seqs,correct_response_seqs), major_fromat_flaw, expected_token_count, response_token_count = grade_response(answer,question_keys_in_order=question_keys_in_order,key_value_dict=key_value_dict, num_facts=num_facts,k=k)\n",
    "    # Print Results\n",
    "    print(\"=== Prompt===\\n\")\n",
    "    print(prompt)\n",
    "    print(\"\\n=== Response ===\")\n",
    "    print(answer)\n",
    "    print(\"\\n=== Correct ===\")\n",
    "    correct_response_text = \"\\n\".join(key_value_dict[key] for key in question_keys_in_order)\n",
    "    print(correct_response_text)\n",
    "    print(\"\\n=== Accuracy ===\")\n",
    "    print(f\"Sequence Accuracy: {response_seqs:.2f}\")\n",
    "    print(f\"Token Accuracy: {correct_response_seqs:.2f}\")\n",
    "    print(f\"Major Format Flaw: {major_fromat_flaw}\")\n",
    "# Uncomment to Run the test\n",
    "#grade_test(single_token_vocab=single_token_vocab, num_facts=5, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_31448\\1832724122.py:14: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saved results grouped by (num_facts, k) in: results/gpt3.5_3k_6f_2t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_experiments(\n",
    "    facts_list_sizes=[3, 6],\n",
    "    token_sizes=[2, 3],\n",
    "    trials=1,\n",
    "    output_dir=\"results\"\n",
    "):\n",
    "    \"\"\"Run GPT response evaluations and save structured JSON per (num_facts, k) combo.\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    single_token_vocab = list(token_set)\n",
    "\n",
    "    for num_facts in facts_list_sizes:\n",
    "        for k in token_sizes:\n",
    "            group_id = f\"{num_facts}facts_{k}tokens\"\n",
    "            timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "            file_id = f\"{group_id}_{timestamp}\"\n",
    "            output_path = Path(output_dir) / f\"{file_id}.json\"\n",
    "\n",
    "            results_group = {\n",
    "                \"id\": file_id,\n",
    "                \"num_facts\": num_facts,\n",
    "                \"k\": k,\n",
    "                \"trials\": []\n",
    "            }\n",
    "\n",
    "            for trial_idx in range(trials):\n",
    "                # Generate data\n",
    "                facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "                prompt_text, question_keys = build_prompt_for_all_keys(facts_list)\n",
    "\n",
    "                try:\n",
    "                    response = query_gpt(prompt_text, model=MODEL_NAME, temperature=0.0)\n",
    "                except Exception as e:\n",
    "                    response = f\"ERROR: {str(e)}\"\n",
    "\n",
    "                correct_response_text = \"\\n\".join(key_value_dict[key] for key in question_keys)\n",
    "\n",
    "                # Grade and evaluate\n",
    "                (seq_acc, tok_acc), major_format_flaw, expected_token_count, response_token_count = grade_response(\n",
    "                    response, question_keys, key_value_dict, num_facts, k\n",
    "                )\n",
    "\n",
    "                trial_record = {\n",
    "                    \"trial\": trial_idx,\n",
    "                    \"sequence_accuracy\": seq_acc,\n",
    "                    \"token_accuracy\": tok_acc,\n",
    "                    \"major_format_flaw\": major_format_flaw,\n",
    "                    \"prompt_text\": prompt_text,\n",
    "                    \"response_text\": response,\n",
    "                    \"response_token_count\": response_token_count,\n",
    "                    \"expected_response_text\": correct_response_text,\n",
    "                    \"expected_token_count\": expected_token_count\n",
    "                }\n",
    "\n",
    "                results_group[\"trials\"].append(trial_record)\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results_group, f, indent=2)\n",
    "\n",
    "    return f\"Saved results grouped by (num_facts, k) in: {output_dir}\"\n",
    "\n",
    "\n",
    "run_experiments(\n",
    "    facts_list_sizes=[3],\n",
    "    token_sizes=[2],\n",
    "    trials=1,\n",
    "    output_dir=\"results/gpt3.5_3k_6f_2t\" \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
