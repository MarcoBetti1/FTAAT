{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfact_gen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_unique_sequence,generate_facts_k_tokens\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuild_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_prompt_for_all_keys\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mquery_counting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m count_gpt_tokens,query_gpt\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Imports\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FTAT/scripts/build_prompt.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfact_gen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquery_counting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_prompt_for_all_keys\u001b[39m(facts_list):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    - Lists all facts in the prompt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    - Then lists all keys in random order, requesting the correct values\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m       (prompt_str, question_keys_in_order)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FTAT/scripts/query_counting.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return how many tokens `text` uses under the GPT-3.5 (or GPT-4) tokenizer.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoding.encode(text))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_gpt\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = \u001b[43mMODEL_NAME\u001b[49m, temperature: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Calls OpenAI's ChatCompletion API with the given prompt,\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    returns the assistant's message content.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     response = openai.ChatCompletion.create(\n\u001b[32m     11\u001b[39m         model=model,\n\u001b[32m     12\u001b[39m         messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m         temperature=temperature\n\u001b[32m     17\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import random\n",
    "import string\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scripts.fact_gen import generate_unique_sequence,generate_facts_k_tokens\n",
    "from scripts.build_prompt import build_prompt_for_all_keys\n",
    "from scripts.query_counting import count_gpt_tokens,query_gpt\n",
    "\n",
    "# Imports\n",
    "openai.api_key = \"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\"\n",
    "# Log Directory\n",
    "LOG_DIR = Path(\"experiment_logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "ENCODING_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "# For token counting\n",
    "encoding = tiktoken.encoding_for_model(ENCODING_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Token Set\n",
    "filename = \"data/tokens/alpha_tokens.json\"\n",
    "with open(filename, 'r') as f:\n",
    "        token_set = set(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(facts_list_sizes=[5, 10], token_sizes=[2, 3], trials=3):\n",
    "    \"\"\"Enhanced version with comprehensive logging\"\"\"\n",
    "    single_token_vocab = list(token_set)\n",
    "    \n",
    "    records = []\n",
    "    for num_facts in facts_list_sizes:\n",
    "        for k in token_sizes:\n",
    "            for trial_idx in range(trials):\n",
    "                # Generate facts and prompt\n",
    "                facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "                prompt_text, question_keys = build_prompt_for_all_keys(facts_list)\n",
    "                prompt_len = count_gpt_tokens(prompt_text)\n",
    "                \n",
    "                # Query GPT\n",
    "                try:\n",
    "                    response = query_gpt(prompt_text, model=MODEL_NAME, temperature=0.0)\n",
    "                except Exception as e:\n",
    "                    response = f\"ERROR: {str(e)}\"\n",
    "                \n",
    "                # Grade response\n",
    "                accuracy, num_matches, total_questions, mismatches = grade_response(\n",
    "                    response, question_keys, key_value_dict\n",
    "                )\n",
    "                \n",
    "                # Build complete record\n",
    "                record = {\n",
    "                    \"num_facts\": num_facts,\n",
    "                    \"k\": k,\n",
    "                    \"trial\": trial_idx,\n",
    "                    \"prompt_text\": prompt_text,  # Store actual prompt\n",
    "                    \"question_keys\": question_keys,  # Store expected questions\n",
    "                    \"key_value_dict\": key_value_dict,  # Store correct answers\n",
    "                    \"prompt_len\": prompt_len,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"num_correct\": num_matches,\n",
    "                    \"num_questions\": total_questions,\n",
    "                    \"mismatches\": mismatches,\n",
    "                    \"response\": response,\n",
    "                }\n",
    "                \n",
    "                # Log everything\n",
    "                #log_experiment(record)\n",
    "                # if accuracy < 1:\n",
    "                #     log_mismatch_details(record)\n",
    "                \n",
    "                records.append(record)\n",
    "                print(f\"Completed: facts={num_facts}, k={k}, trial={trial_idx}, accuracy={accuracy:.2f}\")\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Example usage (Uncomment once you have your API key set):\n",
    "#results_df = run_experiments(facts_list_sizes=[50, 100, 150 ,200, 250,300,350,400,450], token_sizes=[1,2,3,4], trials=4)\n",
    "#results_df = run_experiments(facts_list_sizes=[2,5,10,25,50,100], token_sizes=[1,2,5,10,25], trials=10)\n",
    "#results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-token vocab size: 10000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m num_facts = \u001b[32m5\u001b[39m\n\u001b[32m      7\u001b[39m k = \u001b[32m3\u001b[39m  \u001b[38;5;66;03m# each key = 3 tokens, each value = 3 tokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m facts_list, key_value_dict = \u001b[43mgenerate_facts_k_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_facts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_token_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Build the big prompt that queries all keys in random order\u001b[39;00m\n\u001b[32m     11\u001b[39m prompt, question_keys_in_order = build_prompt_for_all_keys(facts_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FTAT/scripts.py:29\u001b[39m, in \u001b[36mgenerate_facts_k_tokens\u001b[39m\u001b[34m(num_facts, k, single_token_pool)\u001b[39m\n\u001b[32m     26\u001b[39m key_value_dict = {}\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_facts):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     key_seq = \u001b[43mgenerate_unique_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_token_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     val_seq = generate_unique_sequence(k, single_token_pool, used_sequences)\n\u001b[32m     31\u001b[39m     fact_line = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FTAT/scripts.py:8\u001b[39m, in \u001b[36mgenerate_unique_sequence\u001b[39m\u001b[34m(k, single_token_pool, used_sequences)\u001b[39m\n\u001b[32m      6\u001b[39m max_tries = \u001b[32m5000\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tries):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     chosen_tokens = \u001b[43mrandom\u001b[49m.sample(single_token_pool, k=k)\n\u001b[32m      9\u001b[39m     seq = \u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m.join(chosen_tokens)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m seq \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m used_sequences:\n",
      "\u001b[31mNameError\u001b[39m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# EXAMPLE PROMPT AND RESPONSE\n",
    "single_token_vocab = list(token_set)\n",
    "print(\"Single-token vocab size:\", len(single_token_vocab))\n",
    "\n",
    "# Generate some facts\n",
    "num_facts = 5\n",
    "k = 3  # each key = 3 tokens, each value = 3 tokens\n",
    "facts_list, key_value_dict = generate_facts_k_tokens(num_facts, k, single_token_vocab)\n",
    "\n",
    "# Build the big prompt that queries all keys in random order\n",
    "prompt, question_keys_in_order = build_prompt_for_all_keys(facts_list)\n",
    "\n",
    "print(\"=== Prompt (Debug) ===\\n\")\n",
    "print(prompt)\n",
    "prompt_len = count_gpt_tokens(prompt)\n",
    "print(f\"\\nPrompt token length: {prompt_len}\")\n",
    "\n",
    "print(\"\\n=== The correct ordering of values (reference) ===\")\n",
    "for key in question_keys_in_order:\n",
    "    print(f\"{key_value_dict[key]}\")\n",
    "\n",
    "# Query GPT (uncomment if you have your key set up)\n",
    "answer = query_gpt(prompt, model=MODEL_NAME, temperature=0.0)\n",
    "print(\"\\n=== GPT RESPONSE ===\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
