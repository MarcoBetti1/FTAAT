{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f6a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, json, random\n",
    "from typing import Set, Callable, Sequence, List\n",
    "import tiktoken, pathlib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5290eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_utils.py  (or just run in a notebook cell)\n",
    "def generate_alpha_tokens(\n",
    "    N: int,\n",
    "    *,\n",
    "    encode: Callable[[str], list[int]],\n",
    "    decode_single: Callable[[int], str],\n",
    "    alphabet: str = \"abcdefghijklmnopqrstuvwxyz\",\n",
    "    min_len: int = 2,              #  <<<  NEW  ‚Äî exclude 1- and 2-char tokens\n",
    "    max_len: int = 10,\n",
    ") -> Set[str]:\n",
    "    \"\"\"\n",
    "    Return a *set* of N strings such that\n",
    "\n",
    "      ‚Ä¢ they are made only from characters in `alphabet`\n",
    "      ‚Ä¢ their length is in [min_len, max_len]\n",
    "      ‚Ä¢ each is exactly ONE token according to `encode`\n",
    "    \"\"\"\n",
    "\n",
    "    assert min_len >= 1 and max_len >= min_len\n",
    "    tokens: Set[str] = set()\n",
    "\n",
    "    # 1Ô∏è‚É£ deterministic sweep\n",
    "    for length in range(min_len, max_len + 1):\n",
    "        for chars in itertools.product(alphabet, repeat=length):\n",
    "            s = \"\".join(chars)\n",
    "            if len(encode(s)) == 1 and decode_single(encode(s)[0]) == s:\n",
    "                tokens.add(s)\n",
    "                if len(tokens) >= N:\n",
    "                    return tokens\n",
    "\n",
    "    # 2Ô∏è‚É£ random top-up\n",
    "    while len(tokens) < N:\n",
    "        length = random.randint(min_len, max_len + 3)\n",
    "        s = \"\".join(random.choice(alphabet) for _ in range(length))\n",
    "        if (\n",
    "            len(encode(s)) == 1\n",
    "            and decode_single(encode(s)[0]) == s\n",
    "        ):\n",
    "            tokens.add(s)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def save_tokens(tokens: Set[str], path: str):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(sorted(tokens), f)\n",
    "    print(f\"‚úÖ  saved {len(tokens)} tokens  ‚Üí  {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb21b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separator_cleaner.py\n",
    "def _single_tokens(path: str, encode: Callable[[str], Sequence[int]]) -> list[str]:\n",
    "    with open(path) as f:\n",
    "        vocab = json.load(f)\n",
    "    return [t for t in vocab if len(encode(t)) == 1]\n",
    "\n",
    "\n",
    "def _seq_expected_len(seq_len: int, sep_token_len: int) -> int:\n",
    "    #  | t1 | t2 | ‚Ä¶ | tL |\n",
    "    #  ‚Üí  L tokens  +  (L+1) separators\n",
    "    return seq_len + (seq_len + 1) * sep_token_len\n",
    "\n",
    "\n",
    "def _mismatch_stats(\n",
    "    tokens: list[str],\n",
    "    encode: Callable[[str], Sequence[int]],\n",
    "    separator: str,\n",
    "    max_sequence_length: int\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Count how many failing combinations each token participates in\n",
    "    for all sequence lengths 2 ‚Ä¶ max_sequence_length (inclusive).\n",
    "    \"\"\"\n",
    "    sep_len = len(encode(separator))\n",
    "    stats   = Counter()\n",
    "\n",
    "    for L in range(2, max_sequence_length + 1):\n",
    "        for combo in itertools.combinations(tokens, L):\n",
    "            test_str = separator + separator.join(combo) + separator\n",
    "            if len(encode(test_str)) != _seq_expected_len(L, sep_len):\n",
    "                for t in combo:\n",
    "                    stats[t] += 1\n",
    "    return stats\n",
    "\n",
    "\n",
    "def trim_token_set(\n",
    "    json_path: str,\n",
    "    encode: Callable[[str], Sequence[int]],\n",
    "    separator: str = \"|\",\n",
    "    *,\n",
    "    single_surround: bool = True,\n",
    "    max_sequence_length: int = 2,\n",
    "    save_as: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Clean a single-token vocabulary so it is safe to concatenate with `separator`.\n",
    "\n",
    "    ‚Ä¢ `single_surround`   -if True, first removes any token that fails `|token|` (3-token) test  \n",
    "    ‚Ä¢ `max_sequence_length` -exhaustively test sequences of 2 ‚Ä¶ L tokens written\n",
    "                              as `|t1|t2|‚Ä¶|tL|` and trim according to the 'heavy-offender ‚Üí purge' rule\n",
    "    \"\"\"\n",
    "    save_as = save_as or json_path.replace(\".json\", \"_clean.json\")\n",
    "    vocab   = _single_tokens(json_path, encode)\n",
    "    print(f\"Loaded {len(vocab)} single-token strings from {json_path}\")\n",
    "\n",
    "    sep_len = len(encode(separator))\n",
    "\n",
    "    # ---------- SINGLE-SURROUND TEST ----------\n",
    "    if single_surround:\n",
    "        bad = [\n",
    "            t for t in vocab\n",
    "            if len(encode(f\"{separator}{t}{separator}\")) != 1 + 2 * sep_len\n",
    "        ]\n",
    "        if bad:\n",
    "            print(f\"Removing {len(bad)} token(s) that fail the '|token|' test.\")\n",
    "            vocab = [t for t in vocab if t not in bad]\n",
    "        else:\n",
    "            print(\"All tokens pass '|token|' test.\")\n",
    "\n",
    "    # ---------- SEQUENCE TESTS ----------\n",
    "    stats = _mismatch_stats(vocab, encode, separator, max_sequence_length)\n",
    "    if not stats:\n",
    "        print(\"üéâ  No mismatches for sequence tests ‚Äì nothing else to trim.\")\n",
    "        with open(save_as, \"w\") as f:\n",
    "            json.dump(sorted(vocab), f)\n",
    "        print(f\"‚úÖ  Final set has {len(vocab)} tokens  ‚Üí  {save_as}\")\n",
    "        return\n",
    "\n",
    "    # Pass 1 ‚Äî heavy offenders (> minimum count)\n",
    "    threshold   = min(stats.values())\n",
    "    heavy_off   = {tok for tok, cnt in stats.items() if cnt > threshold}\n",
    "    print(f\"Pass-1: removing {len(heavy_off)} heavy-offender token(s) (threshold={threshold}).\")\n",
    "    vocab       = [t for t in vocab if t not in heavy_off]\n",
    "\n",
    "    # Pass 2 ‚Äî purge anything that still shows up in a mismatch\n",
    "    stats2      = _mismatch_stats(vocab, encode, separator, max_sequence_length)\n",
    "    if stats2:\n",
    "        print(f\"Pass-2: removing all {len(stats2)} token(s) still involved in mismatches.\")\n",
    "        vocab = [t for t in vocab if t not in stats2]\n",
    "    else:\n",
    "        print(\"Pass-2: clean ‚Äî no remaining mismatches.\")\n",
    "\n",
    "    with open(save_as, \"w\") as f:\n",
    "        json.dump(sorted(vocab), f)\n",
    "    print(f\"‚úÖ  Final set has {len(vocab)} tokens  ‚Üí  {save_as}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2c86cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  saved 10000 tokens  ‚Üí  tokens/gpt35_tokens.json\n",
      "Loaded 10000 single-token strings from tokens/gpt35_tokens.json\n",
      "Removing 6 token(s) that fail the '|token|' test.\n",
      "üéâ  No mismatches for sequence tests ‚Äì nothing else to trim.\n",
      "‚úÖ  Final set has 9994 tokens  ‚Üí  tokens/gpt35_tokens_clean.json\n",
      "‚úÖ  saved 10000 tokens  ‚Üí  tokens/gpt4o_tokens.json\n",
      "Loaded 10000 single-token strings from tokens/gpt4o_tokens.json\n",
      "Removing 4 token(s) that fail the '|token|' test.\n",
      "üéâ  No mismatches for sequence tests ‚Äì nothing else to trim.\n",
      "‚úÖ  Final set has 9996 tokens  ‚Üí  tokens/gpt4o_tokens_clean.json\n"
     ]
    }
   ],
   "source": [
    "### OPENAI VOCABULARY GENERATION ###\n",
    "def build_openai_vocab(model: str, out_path: str):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    tokens = generate_alpha_tokens(\n",
    "        N=10_000,\n",
    "        encode=enc.encode,\n",
    "        # tiktoken *does* have decode_single_token_bytes\n",
    "        decode_single=lambda tid: enc.decode_single_token_bytes(tid).decode(),\n",
    "        min_len=2, max_len=10,\n",
    "    )\n",
    "    save_tokens(tokens, out_path)\n",
    "    trim_token_set(\n",
    "    out_path,\n",
    "    encode=enc.encode,\n",
    "    separator=\"|\",\n",
    "    single_surround=True,\n",
    "    max_sequence_length=2         #  anything more may take a while\n",
    ")\n",
    "\n",
    "build_openai_vocab(\"gpt-3.5-turbo\",  \"tokens/gpt35_tokens.json\")\n",
    "build_openai_vocab(\"gpt-4o-mini\",    \"tokens/gpt4o_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802bd01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  saved 10000 tokens  ‚Üí  tokens/deepseek_tokens.json\n"
     ]
    }
   ],
   "source": [
    "### DEEP SEEK TOKEN GENERATION ###\n",
    "from deepseek_tokenizer import ds_token            # wheel from pip\n",
    "\n",
    "# helper to adapt the decoder\n",
    "def decode_one(tid: int) -> str:\n",
    "    return ds_token.decode([tid])                  # DeepSeek path\n",
    "\n",
    "tokens = generate_alpha_tokens(\n",
    "    N=10_000,\n",
    "    encode=ds_token.encode,\n",
    "    decode_single=decode_one,      # or just omit & skip equality check\n",
    "    min_len=2,\n",
    "    max_len=10,\n",
    ")\n",
    "save_tokens(tokens, \"tokens/deepseek_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d914dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 single-token strings from tokens/deepseek_tokens.json\n",
      "Removing 1 token(s) that fail the '|token|' test.\n",
      "üéâ  No mismatches for sequence tests ‚Äì nothing else to trim.\n",
      "‚úÖ  Final set has 9999 tokens  ‚Üí  tokens/deepseek_tokens_clean.json\n"
     ]
    }
   ],
   "source": [
    "### DEEP SEEK TOKEN CLEANING ###\n",
    "trim_token_set(\n",
    "    \"tokens/deepseek_tokens.json\",\n",
    "    encode=ds_token.encode,\n",
    "    separator=\"|\",\n",
    "    single_surround=True,\n",
    "    max_sequence_length=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74574924",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown encoding llama3.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.9.0 (are you on latest?)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### LLAMA3 TOKEN GENERATION \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# LLaMA 4 Scout\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ollama run llama4:109b-q4_K_M \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     enc = \u001b[43mtiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# works once tiktoken‚â•0.8 ships it\u001b[39;00m\n\u001b[32m      7\u001b[39m     decode_one = \u001b[38;5;28;01mlambda\u001b[39;00m tid: enc.decode_single_token_bytes(tid).decode()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# ---- fallback to ü§ó Transformers (needs the model files) ----\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Marco\\Desktop\\FTAT\\FTAT\\venv\\Lib\\site-packages\\tiktoken\\registry.py:79\u001b[39m, in \u001b[36mget_encoding\u001b[39m\u001b[34m(encoding_name)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m ENCODING_CONSTRUCTORS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoding_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENCODING_CONSTRUCTORS:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     80\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtiktoken version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (are you on latest?)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m     )\n\u001b[32m     85\u001b[39m constructor = ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[32m     86\u001b[39m enc = Encoding(**constructor())\n",
      "\u001b[31mValueError\u001b[39m: Unknown encoding llama3.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.9.0 (are you on latest?)"
     ]
    }
   ],
   "source": [
    "### LLAMA3 TOKEN GENERATION \n",
    "# LLaMA 4 Scout\n",
    "# ollama run llama4:109b-q4_K_M \n",
    "\n",
    "try:\n",
    "    enc = tiktoken.get_encoding(\"llama3\")   # works once tiktoken‚â•0.8 ships it\n",
    "    decode_one = lambda tid: enc.decode_single_token_bytes(tid).decode()\n",
    "except KeyError:\n",
    "    # ---- fallback to ü§ó Transformers (needs the model files) ----\n",
    "    from transformers import AutoTokenizer\n",
    "    # TODO: replace with the exact model you will download\n",
    "    hf_tok = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", use_fast=True)\n",
    "    enc = hf_tok\n",
    "    decode_one = lambda tid: hf_tok.decode([tid])\n",
    "\n",
    "tokens = generate_alpha_tokens(\n",
    "    N=10_000,\n",
    "    encode=enc.encode if hasattr(enc, \"encode\") else (lambda s: enc(s)[\"input_ids\"]),\n",
    "    decode_single=decode_one,\n",
    "    min_len=3, max_len=10,\n",
    ")\n",
    "save_tokens(tokens, \"tokens/llama3_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc28677",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLAMA3 SEPARATOR TESTING ###\n",
    "analyze_separator(\n",
    "    \"data/tokens/llama3_tokens.json\",\n",
    "    encode=encode,\n",
    "    separator=\"|\",\n",
    "    max_pairs=49995000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
